{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize, PorterStemmer          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "\n",
    "train = pd.read_json('cooking_train.json')\n",
    "test = pd.read_json('cooking_test.json')\n",
    "\n",
    "def joining_preprocessor(line):\n",
    "    return ' '.join(line).lower()\n",
    "\n",
    "def get_word_freq(corpus,n_gram=(1,1)):\n",
    "    vec = CountVectorizer(preprocessor=joining_preprocessor,ngram_range=n_gram).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    return [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "def get_top_n_words(corpus, n=None, n_gram=(1,1)):\n",
    "    words_freq = get_word_freq(corpus,n_gram)\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "def get_not_used_again(train,test):\n",
    "    ps = PorterStemmer()\n",
    "    all_words_train = get_top_n_words(train['ingredients'])\n",
    "    all_words_test = get_top_n_words(test['ingredients'])\n",
    "    list_word_train = [ps.stem(t) for t,freq in all_words_train]\n",
    "    list_word_test = [ps.stem(t) for t,freq in all_words_test]\n",
    "    return list(set(list_word_train) - set(list_word_test))\n",
    "\n",
    "def get_counting_information(pd):\n",
    "    return np.array(pd['ingredients'].str.len()),np.array(pd['ingredients'].apply(lambda x: len(' '.join(x).split(sep=' '))))\n",
    "\n",
    "not_used_again_word = get_not_used_again(train,test)\n",
    "\n",
    "def preprocessor(line):\n",
    "    joining = ' '.join(line).lower()\n",
    "    to_remove = string.digits\n",
    "    to_exchange_with_space = string.punctuation + '\\u00ae' + '\\u2122'\n",
    "    without_punctuation = joining.translate(str.maketrans(to_exchange_with_space,' ' * len(to_exchange_with_space)))\n",
    "    return without_punctuation.translate(str.maketrans('', '', to_remove))\n",
    "\n",
    "# def get_most_popular_ingredients(n):\n",
    "#     unique_ingredients = []\n",
    "#     for arr in train.ingredients:\n",
    "#         unique_ingredients = unique_ingredients + list(set(arr) - set(unique_ingredients))\n",
    "#     \n",
    "#     ingredients = {}\n",
    "#     \n",
    "#     for ing in unique_ingredients:\n",
    "#         ingredients[ing] = 0\n",
    "#     \n",
    "#     for l in train.ingredients:\n",
    "#         for element in l:\n",
    "#             ingredients[element] += 1\n",
    "#     most_ingredients = [(k, ingredients[k]) for k in sorted(ingredients, key=ingredients.get, reverse=True)]\n",
    "#     return most_ingredients[:n]\n",
    "\n",
    "#POMYSŁ Z DORZUCANIEM DO TOKENÓW CAŁYCH PRZEPISÓW\n",
    "\n",
    "# class CustomAnalyzer:\n",
    "#     def __init__(self):\n",
    "#        self.wnl = WordNetLemmatizer()\n",
    "#     def __call__(self, doc):\n",
    "#         tokens =[]\n",
    "#         rest = []\n",
    "#         for receipe in doc:\n",
    "#             if receipe in most_ingredients:\n",
    "#                 tokens.append(receipe)\n",
    "#             else:\n",
    "#                 rest.append(receipe)\n",
    "#         clean_line = preprocessor(rest)\n",
    "#         return tokens +  [self.wnl.lemmatize(t) for t in word_tokenize(clean_line)]\n",
    "    \n",
    "class LemmaTokenizer:\n",
    "        def __init__(self):\n",
    "           self.wnl = WordNetLemmatizer()\n",
    "           self.ps = PorterStemmer()\n",
    "           self.stopwords = stopwords.words('english')\n",
    "        def __call__(self, doc):\n",
    "            stem =  [self.ps.stem(t) for t in word_tokenize(doc) if t not in self.stopwords]\n",
    "            return [t for t in stem  if t not in not_used_again_word]\n",
    "           \n",
    "def preparing_data(data):\n",
    "    vect = TfidfVectorizer(preprocessor=preprocessor, tokenizer=LemmaTokenizer())\n",
    "    words = vect.fit_transform(data['ingredients'])\n",
    "    word_number, ingredients_number = get_counting_information(data)\n",
    "     # hstack((words,word_number[:,None]))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_dataset = train['ingredients']\n",
    "y_dataset = train['cuisine']\n",
    "\n",
    "\n",
    "svc_model = SVC(C=200, kernel='rbf', gamma=1, shrinking=True, tol=0.01, decision_function_shape='ovr')\n",
    "\n",
    "\n",
    "et_pipeline = Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer(preprocessor=preprocessor,tokenizer=LemmaTokenizer())),\n",
    "        ('classifier', svc_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "res = cross_val_score(et_pipeline, X_dataset, y_dataset,verbose=1,n_jobs=2)\n",
    "print(res)\n",
    "sum(res) / len(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('vectorizer',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=<function preprocessor at 0x7f0a97cc4ae8>,\n                                 smooth_idf=True, stop_words=None,\n                                 strip_accents=...\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=<__main__.LemmaTokenizer object at 0x7f0adc278dd8>,\n                                 use_idf=True, vocabulary=None)),\n                ('classifier',\n                 SVC(C=200, break_ties=False, cache_size=200, class_weight=None,\n                     coef0=0.0, decision_function_shape='ovr', degree=3,\n                     gamma=1, kernel='rbf', max_iter=-1, probability=False,\n                     random_state=None, shrinking=True, tol=0.01,\n                     verbose=False))],\n         verbose=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "train = pd.read_json('cooking_train.json')\n",
    "test = pd.read_json('cooking_test.json')\n",
    "X_dataset = train['ingredients']\n",
    "y_dataset = train['cuisine']\n",
    "test = pd.read_json('cooking_test.json')\n",
    "X_test = test['ingredients']\n",
    "\n",
    "et_pipeline.fit(X_dataset, y_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "prediction = et_pipeline.predict(X_test)\n",
    "\n",
    "submission = test.copy()\n",
    "submission['cuisine'] = prediction\n",
    "submission.to_csv('output/svc_one_vs_rest_submision.csv', index=False, columns=['id', 'cuisine'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "Extra Trees starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (ML_JNP)",
   "language": "python",
   "name": "pycharm-62039e1c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}